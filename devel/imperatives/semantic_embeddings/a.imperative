Check to see if this environment variable has been set: AI_GENERATED_DANDISET_SUMMARIES_DIR
If not, raise an exception and tell user to clone https://github.com/magland/ai_generated_dandiset_summaries

////////////////////////////////////////////////////////////////////////////////////////////////////
Here's how the input files are organized.

$AI_GENERATED_DANDISET_SUMMARIES_DIR/
    dandisets/
        000003/
            summary.md
        000004/
            summary.md
        ...

where 000003, 000004, etc. are the dandiset ids.

The output directory is relative to the directory where this script is run.

When you traverse the dandisets directory, be sure to validate that the dandiset id is a 6-digit number, and ignore the directory if it is not (print a warning).

////////////////////////////////////////////////////////////////////////////////////////////////////
For each dandiset, read the summary.md file and then compute the semantic embedding vector of it using

import phil
vec = phil.compute_semantic_embedding_vector(text: str)

Then assemble those vectors in a JSON file called output/dandiset_embeddings.json starting from the directory where this script is run.

The JSON file should look like this:

{
    "000003": [0.1, 0.2, 0.3, ...],
    "000004": [0.2, 0.3, 0.4, ...],
    ...
}

////////////////////////////////////////////////////////////////////////////////////////////////////
Then prepare a second file output/dandiset_ids.json with a list of dandiset ids like this:

['000003', '000004', ...]

////////////////////////////////////////////////////////////////////////////////////////////////////
Then prepare a third file output/dandiset_embeddings.dat with 32-bit floats containing just the vectors liek this:

0.1 0.2 0.3 ...
0.2 0.3 0.4 ...
...

The vectors should be in the same order as the dandiset_ids.json file.

////////////////////////////////////////////////////////////////////////////////////////////////////
Print progress throughout showing which dandiset is being processed and how many dandisets have been processed so far out of the total number of dandisets.
Also print the length of each embedding vector as you go.

At the end print the sizes of the output files in megabytes.

Then write a output/doc.md file describing the contents of the three files. Also point out the length of the embedding vectors.

////////////////////////////////////////////////////////////////////////////////////////////////////
Finally, upload these files to

https://lindi.neurosift.org/tmp/ai_generated_dandiset_summaries/dandiset_embeddings.json
https://lindi.neurosift.org/tmp/ai_generated_dandiset_summaries/embeddings.dat
https://lindi.neurosift.org/tmp/ai_generated_dandiset_summaries/dandiset_ids.txt

using

import phil
phil.upload_file(url: str, filename: str)

////////////////////////////////////////////////////////////////////////////////////////////////////
AND you're going to implement caching. Each time you are going to compute an embedding vector do the following:
* Compute the sha1 of the text.
* Check if there is a file called embeddings_cache/<sha1>.json
* If so, then read the json file and use that as the embedding vector (it will be a list of numbers). Print that there was a cache hit.
* Otherwise, compute the embedding vector and save it to embeddings_cache/<sha1>.json